#importamos librerias

# Importamos numpy para realizar operaciones numéricas eficientes.
import numpy as np

# Pandas nos permitirá trabajar con conjuntos de datos estructurados.
import pandas as pd

# Desde sklearn.model_selection importaremos funciones para dividir conjuntos de datos y realizar validación cruzada.
from sklearn.model_selection import train_test_split, KFold

# Utilizaremos sklearn.preprocessing para preprocesar nuestros datos antes de entrenar modelos de aprendizaje automático.
from sklearn.preprocessing import StandardScaler

# sklearn.metrics nos proporcionará métricas para evaluar el rendimiento de nuestros modelos.
from sklearn.metrics import accuracy_score

# statsmodels.api nos permitirá realizar análisis estadísticos más detallados y estimación de modelos.
import statsmodels.api as sm

# Por último, matplotlib.pyplot nos ayudará a visualizar nuestros datos y resultados.
import matplotlib.pyplot as plt

#Importamos nuestros datos.

datos = pd.read_csv(r"C:\Users\Hp\OneDrive\Documentos\CURSO PYTHON\sample_endi_model_10p.txt", sep= ";")

#Eliminamos valores nulos
datos = datos[~datos["dcronica"].isna()]

datos_quintil_mujer=datos[(datos["sexo"]=="Mujer")&(datos["quintil"])]

####################################################
#estadisticas básicas
####################################################
datos_quintil_mujer.groupby("quintil").size()

###################################################
#MODELO LOGIT
##################################################
#limpiando la base de los valores faltantes
columnas_con_nulos = ['dcronica', 'region', 'n_hijos', 'tipo_de_piso', 'espacio_lavado', 'categoria_seguridad_alimentaria', 'quintil', 'categoria_cocina', 'categoria_agua', 'serv_hig']
datos_limpios = datos.dropna(subset=columnas_con_nulos).copy()

# Comprobando si hay valores no finitos despues de la limpieza
#print("Número de valores no finitos después de la eliminación:")
datos_limpios.isna().sum()

#TRANSFORMACION DE VARIABLES

#transformo la variable quintil en binaria

datos_limpios.loc[:, 'quintil_binario'] = (datos_limpios['quintil'] == 'Quintil 1').astype(int)

# Filtrar los datos para incluir niñas  que tengan valores válidos en la columna 'quintiles'
datos_quintil = datos_limpios[(datos_limpios['sexo'] == 'Mujer') & (datos_limpios['quintil_binario'] == 1)]

print(datos_quintil)

# Seleccionar las variables relevantes
variables = ['n_hijos', 'region', 'sexo', 'condicion_empleo', 'etnia']

# Filtrar los datos para las variables seleccionadas y eliminar filas con valores nulos en esas variables
for i in variables:
    datos_quintil = datos_quintil[~datos_quintil[i].isna()]

#definimos variables categoricas y numericas
variables_categoricas = ['region', 'sexo', 'condicion_empleo']
variables_numericas = ['n_hijos']

#se crea un tranformador para estandarizar las variables numéricas
#se crea una copia de los datos originales

transformador = StandardScaler()
datos_escalados = datos_limpios.copy()

datos_escalados[variables_numericas] = transformador.fit_transform(datos_escalados[variables_numericas])

#convertimos en valores dummies

datos_dummies = pd.get_dummies(datos_escalados, columns=variables_categoricas, drop_first=True)


#definimos las variables para el modelo
X = datos_dummies[['n_hijos', 'sexo_Mujer', 
                   'condicion_empleo_Empleada']]

y = datos_dummies["quintil_binario"]

#definimos los pesos asociados a cada observaciones para
#considerar el diseño muestral
weights = datos_dummies['fexp_nino']

#dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(X, y, weights, test_size=0.2, random_state=42)

# Convertimos todas las variables a tipo numérico
X_train = X_train.apply(pd.to_numeric, errors='coerce')
y_train = y_train.apply(pd.to_numeric, errors='coerce')
y_train = y_train.apply(pd.to_numeric, errors='coerce')

# Convertimos las variables a tipo entero
variables = X_train.columns
for i in variables:
    X_train[i] = X_train[i].astype(int)
    X_test[i] = X_test[i].astype(int)

y_train = y_train.astype(int)

####################################################
#ajuste del modelo
####################################################

modelo = sm.Logit(y_train, X_train)
result=modelo.fit()
print(result.summary())

#####################################################
#extraer estos coeficientes y organizarlos en un DataFrame
####################################################
# Extraemos los coeficientes y los almacenamos en un DataFrame
coeficientes = result.params
df_coeficientes = pd.DataFrame(coeficientes).reset_index()
df_coeficientes.columns = ['Variable', 'Coeficiente']

# Creamos una tabla pivote para una mejor visualización
df_pivot = df_coeficientes.pivot_table(columns='Variable', values='Coeficiente')
df_pivot.reset_index(drop=True, inplace=True)

print(df_pivot)
##################################################
"""
Pregunta
1. ¿Cuál es el valor del parámetro asociado a la variable 
clave si ejecutamos el modelo solo con el conjunto
 de entrenamiento y predecimos con el mismo conjunto 
 de entrenamiento? ¿Es significativo?
 
el valor del parámetro asociado a la variable 
clave es el valor de -0.8175 asociado a la variable sexo_Mujer
y el p-valor asociado a la variable es 0.000 por lo que es significativo

2. como el coeficiente asociado a la variable sexo_Mujer es negativo, entonces el ser mujer disminuye las probabilidades de pertenecer al quintil de interes
en comparación con el quintil de referencia, manteniendo todas las demás variables constantes.
"""
###################################################
#EJERCICIO 3
#####################################################



# 100 folds:
kf = KFold(n_splits=100)
accuracy_scores = []
df_params = pd.DataFrame()

for train_index, test_index in kf.split(X_train):

    # aleatorizamos los folds en las partes necesarias:
    X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]
    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]
    weights_train_fold, weights_test_fold = weights_train.iloc[train_index], weights_train.iloc[test_index]
    
    # Ajustamos un modelo de regresión logística en el pliegue de entrenamiento
    log_reg = sm.Logit(y_train_fold, X_train_fold)
    result_reg = log_reg.fit()
    
    # Extraemos los coeficientes y los organizamos en un DataFrame
    coeficientes = result_reg.params
    df_coeficientes = pd.DataFrame(coeficientes).reset_index()
    df_coeficientes.columns = ['Variable', 'Coeficiente']
    df_pivot = df_coeficientes.pivot_table(columns='Variable', values='Coeficiente')
    df_pivot.reset_index(drop=True, inplace=True)
    
    # Realizamos predicciones en el pliegue de prueba
    predictions = result_reg.predict(X_test_fold)
    predictions = (predictions >= 0.5).astype(int)
    
    # Calculamos la precisión del modelo en el pliegue de prueba
    accuracy = accuracy_score(y_test_fold, predictions)
    accuracy_scores.append(accuracy)
    
    # Concatenamos los coeficientes estimados en cada pliegue en un DataFrame
    df_params = pd.concat([df_params, df_pivot], ignore_index=True)


print(f"Precisión promedio de validación cruzada: {np.mean(accuracy_scores)}")

# Calcular la precisión promedio
precision_promedio = np.mean(accuracy_scores)

###################################################
#3. ¿Qué sucede con la precisión promedio del modelo cuando se utiliza el conjunto de datos filtrado?
#la precision promedio del modelo baja 
#la precision promedio del modelo filtrado es: 0.6656617647
#mientras que la precision del modelo del ejercicio anterior es: 0.7312725490

#- disminuye en 0.0656107843
###################################################


plt.hist(accuracy_scores, bins=30, edgecolor='black')
# Añadir una línea vertical en la precisión promedio
plt.axvline(precision_promedio, color='red', linestyle='dashed', linewidth=2)

# Añadir un texto que muestre la precisión promedio
plt.text(precision_promedio-0.1, plt.ylim()[1]-0.1, f'Precisión promedio: {precision_promedio:.2f}', 
         bbox=dict(facecolor='white', alpha=0.5))

plt.title('Histograma de Accuracy Scores')
plt.xlabel('Accuracy Score')
plt.ylabel('Frecuencia')

# Ajustar los márgenes
plt.tight_layout()

# Mostrar el histograma
plt.show()


plt.hist(df_params["n_hijos"], bins=30, edgecolor='black')

# Añadir una línea vertical en la media de los coeficientes
plt.axvline(np.mean(df_params["n_hijos"]), color='red', linestyle='dashed', linewidth=2)

# Añadir un texto que muestre la media de los coeficientes
plt.text(np.mean(df_params["n_hijos"])-0.1, plt.ylim()[1]-0.1, f'Media de los coeficientes: {np.mean(df_params["n_hijos"]):.2f}', 
         bbox=dict(facecolor='white', alpha=0.5))

plt.title('Histograma de Beta (N Hijos)')
plt.xlabel('Valor del parámetro')
plt.ylabel('Frecuencia')

# Ajustar los márgenes
plt.tight_layout()

# Mostrar el histograma
plt.show()

###################################################
"""
4. ¿Qué sucede con la distribución de los coeficientes
 beta en comparación con el ejercicio anterior? 

- la distrubucion de los coeficientes beta esta entre
 0.67 en los datos filtrados
 - la distrubucion de los coeficientes beta esta entre
 0.73 en los el ejercicio anterior

 las disminución es de 0.06
"""
###################################################
